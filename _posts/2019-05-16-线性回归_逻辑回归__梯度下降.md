---
layout:     post
title:      线性回归_逻辑回归__梯度下降
subtitle:   随便瞎写的 基础不太好
date:       2019-05-16
author:     Nikola
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - 机器学习
    
    
---

​                                                         **线性回归**

**例子：**

去银行贷款，银行会根据每个人的年龄和工资的多少，来判断给我们多少钱。就像下面的图

​                      ![img](https://i.loli.net/2019/01/25/5c4ae1eebc651.png)

我们把工资、年龄设为x1、x2,把额度设为y。图中数据每行是一个样本。

然后我们会考虑，工资和年龄都会影响最终的额度，那么他们的影响程度会多大？所以，我们就设成了

![img](https://i.loli.net/2019/01/25/5c4ae1ef27746.png)

变换公式：

​                    ![img](https://i.loli.net/2019/01/25/5c4ae1ee99a83.png)

这个公式的意思就是：我们默认x0,x1,x2列向量，并且默认x0值为1。这是了好计算。我们再把θ0，θ1，θ2所在的列向量转置为行向量。然后矩阵计算，因为计算的复杂度通常都比较高，采用矩阵计算效率高，而采用迭代效率就低了。



​                      ![img](https://i.loli.net/2019/01/25/5c4ae1eee6374.png)

误差：

​           ![img](https://i.loli.net/2019/01/25/5c4ae1eeb178d.png)

上面的公式只是计算一个样本，下面是对每个样本的考虑。



​                          ![img](https://i.loli.net/2019/01/25/5c4ae1ef3a026.png)

这其中的i就是样本的编号，y是真实值。x是个列向量（就是一列数据）。
接下来整张截图

![img](https://i.loli.net/2019/01/25/5c4ae1efb19b3.png)

如果有一万个样本，那么就有一万个误差。我们认为每个误差独立并且具有相同分布，而且服从均值为0方差为σ方的高斯分布。
将误差带入高斯分布的公式，这里P就是概率。

​                               ![img](https://i.loli.net/2019/01/25/5c4ae1ef5033a.png)

根据上面的高斯分布图，我们知道误差越小，P就越大。
将

​                                        ![img](https://i.loli.net/2019/01/25/5c4ae1ef699bd.png)

带入得

​                ![img](https://i.loli.net/2019/01/25/5c4ae1efa7911.png)

接下来我们要用似然函数了。
首先似然函数就是用数据去找对应的规则，就是通过数据去推参数等于什么。

![img](https://i.loli.net/2019/01/25/5c4ae278c7073.png)

（这里我用我自己的大白话来解释： 就是将所有的样本对应的P（概率）进行累乘，我们这是考虑到所有的样本了，我们希望这个函数的结果越大越好，因为概率越大，误差越小
误差小了，真实值y越接近，看下面）

​                                    ![img](https://i.loli.net/2019/01/25/5c4ae1ef699bd.png)

接下来，在对似然函数进行变形，首先累乘很难算，我们对函数log进行变形成了对数似然。

![img](https://i.loli.net/2019/01/25/5c4ae1fd0c9f4.png)

接下来数学就容易了，展开：

![img](https://i.loli.net/2019/01/25/5c4ae1fd03604.png)

这很简单，不解释。
我们要是这个目标函数最大，就应该让

​                               ![img](https://i.loli.net/2019/01/25/5c4ae1fcd7d04.png)

最小。

所以，最小二乘法就推导出来了。（那些不用的常数可以去掉）

![img](https://i.loli.net/2019/01/25/5c4ae1fcdee27.png)

然后就是算：

![img](https://i.loli.net/2019/01/25/5c4ae1fdaf248.png)

来解释一一下等式右边，首先矩阵的平方等于矩阵的转置再乘于自身。
然后，看看我举的例子

​                   ![img](https://i.loli.net/2019/01/25/5c4ae1fe32cce.png)

是不是就懂了，是不是一图日破天际。



![img](http://blog3-1258612035.cos.ap-shanghai.myqcloud.com/3/20190203110948663.png)

然后就求导找极值点。求导的过程，要说下第二行到第三行的求导（其实这里我不会求，但是可以套公式）
参考这个链接<https://blog.csdn.net/lirika_777/article/details/79646453>
（数学太差是硬伤，哪天在补补数学）

![img](https://i.loli.net/2019/01/25/5c4ae1fdd214b.png)

到这，θ求出来了。但是它其实有时候并不完全可解。那就要用到梯度下降了 。

​                                                         **梯度下降**



### 为什么用梯度下降

上次我们推导出了θ这个向量，但是它不一定完全可解，所以不能直接就出来了，那就需要一步一步向这个解逼近了。这就是梯度下降。
一般来说我们得到一个损失函数，就像上次我们得到的是：

![img](https://i.loli.net/2019/01/29/5c4ffd5a6d199.png)

在这个式子上再除以m,为什么除以m?
因为我们希望得到一个损失函数，如果不除以m,一万个样本的目标函数的损失值肯定比一千个目标函数大，难道就说十万个样本的模型不好吗？所以这里取个平均得到这个式子：

​                 ![img](http://blog3-1258612035.cos.ap-shanghai.myqcloud.com/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%8E%9F%E7%90%86/20190312074204042.png)

这个就是我们的目标函数（损失函数）。



### 原理

首先，大致图是这样的：

![img](https://i.loli.net/2019/01/29/5c4ffd5bde282.png)

这里是最简单的二维平面，这里参数只有一个。但是如果是多维空间的时候，就是目标J分别对多个参数分别求导了。
现以二维平面为例，再函数中取一个点，求导，以下面图上中间的点为例，它的导数是负值，如果再取一个向右的点，J会减少，取一个在它左边的一个点J会增大。



![img](https://i.loli.net/2019/01/29/5c4ffd5c9536b.png)

叫做梯度下降而不叫上升：        ![img](https://i.loli.net/2019/01/29/5c4ffd5d52bf3.png)   

所以θ：=θ-αJ' 这里(α>0)

对于某个样本它的梯度是这个



![img](https://i.loli.net/2019/01/29/5c4ffd5b449b4.png)

接下来就是公式了：

![img](https://i.loli.net/2019/01/29/5c4ffd5be4722.png)

​                                                                      



​                                                        **逻辑回归**

**介绍**

逻辑回归不是回归，它是一个经典的二分类算法。逻辑回归的决策边界可以是非线性的也就是高阶的。



### Sigmoid函数





公式：

​                                                 ![img](http://blog3-1258612035.cos.ap-shanghai.myqcloud.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/20190312080403044.png)

自变量为任意实数，值域[0,1]再看它的图象：

​                  ![img](http://blog3-1258612035.cos.ap-shanghai.myqcloud.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/20190312081623222.png)

完成由值到概率的转换。



那上次的回归算法为例：

​                                        ![img](https://i.loli.net/2019/01/31/5c52d2a4ab2a4.png)

然后开始分类任务：

​                                          ![img](https://i.loli.net/2019/01/31/5c52d2a4bc998.png)   ![img](https://i.loli.net/2019/01/31/5c52d2a5941cd.png)

解释一下：

![img](https://i.loli.net/2019/01/31/5c52d2a5b148b.png)

然后它的决策边界可能是这样的（这图是胡吊预测的）：

![img](https://i.loli.net/2019/01/31/5c52d2a630cf4.png)

### 似然函数

![img](https://i.loli.net/2019/01/31/5c52d3321dead.png)

变换成对数似然：

![img](https://i.loli.net/2019/01/31/5c52d2a652ad3.png)

这时候，我们就希望目标函数越大越好了，此时应用梯度上升求最大值：

![img](https://i.loli.net/2019/01/31/5c52d2a6420bc.png)

然后就是求导的过程：

![img](https://i.loli.net/2019/01/31/5c52d3395b50c.png)

求导求完了，该参数更新了：

​                                            ![img](https://i.loli.net/2019/01/31/5c52d33203014.png)

原理都到这里。

![img](http://blog3-1258612035.cos.ap-shanghai.myqcloud.com/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/20190312080115030.png)